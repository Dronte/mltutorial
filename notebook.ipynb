{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e6335977-1dcb-a5bc-4856-184dd7bce3f9"
   },
   "source": [
    "## Trying out a linear model: \n",
    "\n",
    "Based on: https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models/code\n",
    "Author: Alexandru Papiu    \n",
    "If you use parts of this notebook in your own scripts, please give some sort of credit (for example link back to this). Thanks!\n",
    "\n",
    "\n",
    "There have been a few [great](https://www.kaggle.com/comartel/house-prices-advanced-regression-techniques/house-price-xgboost-starter/run/348739)  [scripts](https://www.kaggle.com/zoupet/house-prices-advanced-regression-techniques/xgboost-10-kfolds-with-scikit-learn/run/357561) on [xgboost](https://www.kaggle.com/tadepalli/house-prices-advanced-regression-techniques/xgboost-with-n-trees-autostop-0-12638/run/353049) already so I'd figured I'd try something simpler: a regularized linear regression model. Surprisingly it does really well with very little feature engineering. The key point is to to log_transform the numeric variables since most of them are skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0d706811-b70c-aeab-a78b-3c7abd9978d3",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd # for data storing\n",
    "import numpy as np # numeric staff\n",
    "import seaborn as sns # making plots beautifull\n",
    "import matplotlib # for plotting\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "\n",
    "%config InlineBackend.figure_format = 'png' #set 'png' here when working on notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data at: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "603292c1-44b7-d72a-5468-e6782f311603",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d646bb1b-56c4-9b45-d5d4-27095f61b1c0",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cb2d88d7-7f76-4b04-d28b-d2c315ae4346",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                      test.loc[:,'MSSubClass':'SaleCondition']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_features = all_data.dtypes[all_data.dtypes != \"object\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.get_dummies(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = all_data.fillna(all_data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import shuffle\n",
    "all_data_scaled = pd.DataFrame(scale(all_data), index = all_data.index, columns=all_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating matrices for sklearn:\n",
    "X_train = all_data_scaled[:train.shape[0]]\n",
    "\n",
    "X_test = all_data_scaled[train.shape[0]:]\n",
    "y = np.log1p(train.SalePrice)\n",
    "\n",
    "X_train, y = shuffle(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cc4e3014-23b7-2971-ddb0-f67b03f83558"
   },
   "source": [
    "### Models\n",
    "\n",
    "Now we are going to use regularized linear regression models from the scikit learn module. I'm going to try both l_1(Lasso) and l_2(Ridge) regularization. I'll also define a function that returns the cross-validation rmse error so we can evaluate our models and pick the best tuning par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "82886739-eee6-5d7a-4be9-e1fe6ac059f1",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, Lasso\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import get_scorer \n",
    "\n",
    "def rmse_cv(model, data = X_train):\n",
    "    train_rmse = []\n",
    "    test_rmse = []\n",
    "    scorer = get_scorer(\"neg_mean_squared_error\")\n",
    "    for train_index, test_index in KFold(len(y), n_folds=5):\n",
    "        model.fit(data.loc[train_index], y[train_index])\n",
    "        train_rmse.append(np.sqrt(-scorer(model, data.loc[train_index], y[train_index])))\n",
    "        test_rmse.append(np.sqrt(-scorer(model, data.loc[test_index], y[test_index])))\n",
    "    return(np.array(train_rmse), np.array(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "436ce6e8-917f-8c88-3d7b-245e82a1619f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ridge = Ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "69ff958c-dbbb-4750-3fb0-d0ac17ff6363"
   },
   "source": [
    "The main tuning parameter for the Ridge model is alpha - a regularization parameter that measures how flexible our model is. The higher the regularization the less prone our model will be to overfit. However it will also lose flexibility and might not capture all of the signal in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_ridge.fit(X_train, y)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(y, model_ridge.predict(X_train), 'o')\n",
    "plt.xlabel(\"true log(price)\", fontsize=16)\n",
    "plt.ylabel(\"predicted log(price)\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f6b86166-f581-6e05-5274-d3d3516ebaf3",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ridge_alphas = [0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75, 100, 125,\n",
    "              150, 200, 225, 250, 275, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300,1400, 1500]\n",
    "train_cv_ridge, test_cv_ridge = zip(*[rmse_cv(Ridge(alpha = alpha))\n",
    "            for alpha in ridge_alphas])\n",
    "cv_ridge_mean = np.array(map(lambda x:x.mean(), test_cv_ridge))\n",
    "cv_ridge_std = np.array(map(lambda x:x.std(), test_cv_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f8cf53ba-8441-9233-b7f5-a851d270b770",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(ridge_alphas, cv_ridge_mean)\n",
    "#plt.fill_between(ridge_alphas, cv_ridge_mean-cv_ridge_std, cv_ridge_mean+cv_ridge_std, alpha=0.4)\n",
    "\n",
    "plt.plot(ridge_alphas, map(lambda x:x.mean(), train_cv_ridge))\n",
    "plt.legend(['test_score', 'train_score'])\n",
    "plt.xlabel(\"alpha\", fontsize=16)\n",
    "plt.ylabel(\"rmse\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "37486402-4a48-912f-84ee-a3611334b133"
   },
   "source": [
    "Note the U-ish shaped curve above. When alpha is too large the regularization is too strong and the model cannot capture all the complexities in the data. If however we let the model be too flexible (alpha small) the model begins to overfit. A value of alpha = 10 is about right based on the plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d42c18c9-ee70-929f-ce63-aac7f77796cc",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_ridge_mean.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8204520c-a595-2ad2-4685-0b84cc662b84",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasso_alphas = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.02]\n",
    "train_cv_lasso, test_cv_lasso = zip(*[rmse_cv(Lasso(alpha = alpha))\n",
    "                                     for alpha in lasso_alphas])\n",
    "cv_lasso_mean = np.array(map(lambda x:x.mean(), test_cv_lasso))\n",
    "cv_lasso_std = np.array(map(lambda x:x.std(), test_cv_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(lasso_alphas, cv_lasso_mean)\n",
    "plt.fill_between(lasso_alphas, cv_lasso_mean-cv_lasso_std, cv_lasso_mean+cv_lasso_std, alpha=0.4)\n",
    "\n",
    "plt.plot(lasso_alphas, map(lambda x:x.mean(), train_cv_lasso))\n",
    "plt.legend(['test_rmse', 'train_rmse'])\n",
    "plt.xlabel(\"alpha\", fontsize=16)\n",
    "plt.ylabel(\"rmse\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "863fb699-7bcd-3748-3dbb-1c9b18afee9b"
   },
   "source": [
    "So for the Ridge regression we get a rmsle of about 0.127\n",
    "\n",
    "Let' try out the Lasso model. We will do a slightly different approach here and use the built in Lasso CV to figure out the best alpha for us. For some reason the alphas in Lasso CV are really the inverse or the alphas in Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lasso = Lasso(alpha = 0.005).fit(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e78e6126-4de0-08ad-250b-46a3f0f48de0",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmse_cv(model_lasso)[1].mean(), rmse_cv(model_lasso)[1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c7be87ca-412a-cb19-1524-cd94cf698d44",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coef = pd.Series(model_lasso.coef_, index = X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "14be641e-bbe0-824d-d90f-f47698c8b5c5",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ca153134-b109-1afc-e066-44273f65d44c"
   },
   "source": [
    "Good job Lasso.  One thing to note here however is that the features selected are not necessarily the \"correct\" ones - especially since there are a lot of collinear features in this dataset. One idea to try here is run Lasso a few times on boostrapped samples and see how stable the feature selection is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "632e23a8-948f-c692-f5e9-1aa8a75f21d5"
   },
   "source": [
    "We can also take a look directly at what the most important coefficients are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3efc02df-c877-b1fe-1807-5dd93c896c63",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp_coef = pd.concat([coef.sort_values().head(10),\n",
    "                     coef.sort_values().tail(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "87317789-6e7e-d57f-0b54-d8ba0ee26abf",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients in the Lasso Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f6e6f820-1ec6-4a69-c309-9992b80652da"
   },
   "source": [
    "The most important positive feature is `GrLivArea` -  the above ground area by area square feet. This definitely sense. Then a few other  location and quality features contributed positively. Some of the negative features make less sense and would be worth looking into more - it seems like they might come from unbalanced categorical variables.\n",
    "\n",
    " Also note that unlike the feature importance you'd get from a random forest these are _actual_ coefficients in your model - so you can say precisely why the predicted price is what it is. The only issue here is that we log_transformed both the target and the numeric features so the actual magnitudes are a bit hard to interpret. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4780532e-2815-e355-9a96-fb1c598f6984"
   },
   "source": [
    "The residual plot looks pretty good.To wrap it up let's predict on the test set and submit on the leaderboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_modifications_to_numeric(df):\n",
    "    for feature in numeric_features:\n",
    "        #print df[feature].min()\n",
    "        #df[feature+'_log1plog1p'] = np.log1p(np.log1p(df[feature]-df[feature].min()))\n",
    "        df[feature+'_log1p'] = np.log1p(df[feature]-df[feature].min())\n",
    "        #df[feature+'_sqrt'] = np.sqrt(df[feature]-df[feature].min())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data_modified = pd.DataFrame(scale(add_modifications_to_numeric(all_data).as_matrix()),\n",
    "                                 index = all_data.index, columns=all_data.columns)\n",
    "\n",
    "X_train_new = all_data_modified[:train.shape[0]]\n",
    "X_test_new = all_data_modified[train.shape[0]:]\n",
    "del all_data_modified # to save RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8204520c-a595-2ad2-4685-0b84cc662b84",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasso_alphas = [0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.02]\n",
    "train_cv_lasso, test_cv_lasso = zip(*[rmse_cv(Lasso(alpha = alpha), data=X_train_new)\n",
    "                                    for alpha in lasso_alphas])\n",
    "cv_lasso_mean = np.array(map(lambda x:x.mean(), test_cv_lasso))\n",
    "cv_lasso_std = np.array(map(lambda x:x.std(), test_cv_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(lasso_alphas, cv_lasso_mean)\n",
    "plt.fill_between(lasso_alphas, cv_lasso_mean-cv_lasso_std, cv_lasso_mean+cv_lasso_std, alpha=0.4)\n",
    "plt.xlabel(\"alpha\", fontsize=16)\n",
    "plt.ylabel(\"rmse\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lasso = Lasso(alpha = 0.005).fit(X_train_new, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c7be87ca-412a-cb19-1524-cd94cf698d44",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coef = pd.Series(model_lasso.coef_, index = X_train_new.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "14be641e-bbe0-824d-d90f-f47698c8b5c5",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8da43e0-fd51-a4c9-d9b2-364d9911699a"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ae9bcc1a-5106-0909-ecf7-0abc2d2ca386"
   },
   "source": [
    "Let's add an xgboost model to our linear model to see if we can improve our score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "654e4fcf-a049-921a-4783-3c6d6dcca673",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_depths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "train_cv_rf, test_cv_rf = zip(*[rmse_cv(RandomForestRegressor(n_estimators=100, max_depth=depth))\n",
    "                                     for depth in max_depths])\n",
    "cv_rf_mean = np.array(map(lambda x:x.mean(), test_cv_rf))\n",
    "cv_rf_std = np.array(map(lambda x:x.std(), test_cv_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(max_depths, cv_rf_mean)\n",
    "plt.fill_between(max_depths, cv_rf_mean-cv_rf_std, cv_rf_mean+cv_rf_std, alpha=0.4)\n",
    "plt.plot(max_depths, map(lambda x:x.mean(), train_cv_rf))\n",
    "plt.xlabel(\"depth\", fontsize=16)\n",
    "plt.ylabel(\"rmse\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_features_list = range(10,200,3)\n",
    "train_cv_rf, test_cv_rf = zip(*[rmse_cv(RandomForestRegressor(n_estimators=100,\n",
    "                                                              max_depth=12,\n",
    "                                                              min_samples_split = 2,\n",
    "                                                              max_features = max_features, n_jobs=3))\n",
    "                                     for max_features in max_features_list])\n",
    "cv_rf_mean = np.array(map(lambda x:x.mean(), test_cv_rf))\n",
    "cv_rf_std = np.array(map(lambda x:x.std(), test_cv_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(max_features_list, cv_rf_mean)\n",
    "plt.fill_between(max_features_list, cv_rf_mean-cv_rf_std, cv_rf_mean+cv_rf_std, alpha=0.4)\n",
    "plt.plot(max_features_list, map(lambda x:x.mean(), train_cv_rf))\n",
    "plt.xlabel(\"\", fontsize=16)\n",
    "plt.ylabel(\"rmse\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1c9c640b-9e6c-a350-0691-7f6a7dc19c41",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame({\"xgb\":xgb_preds, \"lasso\":lasso_preds})\n",
    "predictions.plot(x = \"xgb\", y = \"lasso\", kind = \"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "74c9bdd2-afbb-5fdf-a776-a8eebfa30d12"
   },
   "source": [
    "Many times it makes sense to take a weighted average of uncorrelated results - this usually imporoves the score although in this case it doesn't help that much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "623ed0fe-0150-5226-db27-90a321061d52",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "569d7154-e3b5-84ab-1d28-57bdc02955d9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "solution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":preds})\n",
    "solution.to_csv(\"ridge_sol.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 5,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
